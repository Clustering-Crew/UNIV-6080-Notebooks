{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Clustering-Crew/UNIV-6080-Notebooks/blob/main/EigendecompositionAndDiagonalization_With_Exercise_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h1IyDdBVM14"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import inv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovZaM9JiVR3c"
      },
      "source": [
        "# Eigendecomposition and Diagonalization\n",
        "\n",
        "Let's work through Example 4.11 using NumPy.\n",
        "\n",
        "We would like to compute the eigendecomposition of\n",
        "$\n",
        "\\mathbf{A} =\n",
        "\\begin{bmatrix}\n",
        "2 & 1\\\\\n",
        "1 & 2\n",
        "\\end{bmatrix}\n",
        "$.\n",
        "\n",
        "Step 1 is to compute eigenvalues and eigenvectors. Instead of using the characteristic polynomial, we will use `numpy.eig`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhZtETUdV4aA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c2002b-46d1-468b-836b-9acf17cd6c67"
      },
      "source": [
        "A = np.array([[2, 1], [1, 2]])\n",
        "d, P = np.linalg.eig(A)\n",
        "print(d)\n",
        "print(P)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3. 1.]\n",
            "[[ 0.70710678 -0.70710678]\n",
            " [ 0.70710678  0.70710678]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztYrl7e6WEcJ"
      },
      "source": [
        "Step 2 is to check for existence. The eigenvectors\n",
        "$\n",
        "\\frac{1}{\\sqrt{2}}\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "-1\n",
        "\\end{bmatrix}\n",
        "$\n",
        "and\n",
        "$\n",
        "\\frac{1}{\\sqrt{2}}\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "$\n",
        "form a basis of $\\mathbb{R}^2$. Therefore $\\mathbf{A}$ can be diagonalized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGLFF2RkW9Gw"
      },
      "source": [
        "Step 3 is to construct the matrix $P$ to diagonalize $A$. `numpy.eig` already returns the eigenvectors as the columns of a matrix, so we are good to go. Let's form $\\mathbf{D} = \\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYu7pCzmXTro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f898009-ae5e-44c9-f8d9-32a47ef1189e"
      },
      "source": [
        "D = np.linalg.multi_dot([inv(P), A, P])\n",
        "print(D)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3.00000000e+00 -1.01465364e-17]\n",
            " [-3.04396091e-17  1.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBr9Tsb_XeGP"
      },
      "source": [
        "## Geometric intuition for the eigendecomposition\n",
        "\n",
        "We have successfully diagonalized $\\mathbf{A}$. Let's verify that applying the linear mapping represented by $\\mathbf{A}$ to a vector $\\mathbf{x} =\n",
        "\\begin{bmatrix}\n",
        "1 & 1\n",
        "\\end{bmatrix}^{\\top}\n",
        "$ is equivalent to first applying a change of basis, then scaling, then undoing the change of basis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBL4AqyxYW_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d4ac55-082a-47a1-8f35-8b51bb4d250d"
      },
      "source": [
        "x = np.array([1, 1])\n",
        "np.dot(A, x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpIsUswIYzOk"
      },
      "source": [
        "First, the change of basis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjtsXcGlYoCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "165505aa-c5f2-4140-ff2c-1c87ddb0087a"
      },
      "source": [
        "b = np.dot(inv(P), x)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.41421356 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTtVhTZ2Y2Vu"
      },
      "source": [
        "Now the scaling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJstbfg1Y97Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "915895e4-729f-4fe4-8783-ed3dfb6f664f"
      },
      "source": [
        "b_scaled = np.dot(D, b)\n",
        "print(b_scaled)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 4.24264069e+00 -4.30481080e-17]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqEHyENOZL5f"
      },
      "source": [
        "Now undoing the change of basis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlkX2RDiZQBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bbc5af3-eca8-4dd1-f97d-ea1a2d4a17ff"
      },
      "source": [
        "np.dot(P, b_scaled)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEEvZJ-eZU1O"
      },
      "source": [
        "We see that this is equivalent to the linear mapping with $\\mathbf{A}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFBc6ut8ZaVf"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "We read that if the eigendecomposition exists, then the determinant can easily be computed as\n",
        "\n",
        "$$\n",
        "\\det(\\mathbf{A}) = \\det(\\mathbf{PDP}^{-1})=\\det(\\mathbf{P})\\det(\\mathbf{D})\\det(\\mathbf{P}^{-1})\n",
        "$$\n",
        "\n",
        "Since $\\mathbf{D}$ is diagonal, we know that its determinant is simply the product of its diagonal elements. But why is it easy to compute the determinant of $\\mathbf{P}$ or $\\mathbf{P}^{-1}$?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer\n",
        "\n",
        "From equation 4.63a and 4.63b, the equation can be further simplified into just $\\det(\\mathbf{D})$ because, since $\\mathbf{P}$ is an orthogonal matrix, therefore $\\det(\\mathbf{P}^{-1}) = \\dfrac{1}{\\det(\\mathbf{P})}$ and they cancel out each other."
      ],
      "metadata": {
        "id": "-GAWONoryiRE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnLeTZeq0zH-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}